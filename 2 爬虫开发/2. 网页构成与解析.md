## 网页基础

用浏览器

### 网页构成

介绍网页的组成

### 正则表达式
python 内置re支持

### XPath
速度快 支持xml解析
```bash
pip install lxml
```
#### XPathDemo
``` python
from lxml import etree
# 字符串读取html
html_str = '''  
<div>
    <ul>
         <li class="item-0"><a href="link1.html">first item</a></li>
         <li class="item-1"><a href="link2.html">second item</a></li>
         <li class="item-inactive"><a href="link3.html"><span class="bold">third item</span></a></li>
         <li class="item-1"><a href="link4.html">fourth item</a></li>
         <li class="item-0"><a href="link5.html">fifth item</a></li>
     </ul>
 </div>


html = etree.HTML(html_str)
#也可以从文件读取html html = etree.parse('hello.html') 

res = html.xpath("//div/ul/li[@class='item-0']/a")

for e in res:
    print(e.text)
    print(e.get("href"))
```

#### xpath语法
// 用于获取所有子孙节点 / 用于获取直接子节点 .代表当前节点
```
1. 获取 <li> 标签的所有 class
result = html.xpath('//li/@class')
print result
运行结果:
['item-0', 'item-1', 'item-inactive', 'item-1', 'item-0']

2. 获取 <li> 标签下 href 为 link1.html 的 <a> 标签
result = html.xpath('//li/a[@href="link1.html"]')
print result
运行结果
[<Element a at 0x10ffaae18>]

3. 获取 <li> 标签下的所有 <span> 标签
不能这样写：result = html.xpath('//li/span')
因为 / 是用来获取子元素的，而 <span> 并不是 <li> 的子元素，所以，要用双斜杠

result = html.xpath('//li//span')
print result
运行结果
[<Element span at 0x10d698e18>]

4. 获取 <li> 标签下的所有 class，不包括 <li>
result = html.xpath('//li/a//@class')
print result
运行结果
['blod']

5. 获取最后一个 <li> 的 <a> 的 href
result = html.xpath('//li[last()]/a/@href')
print result
运行结果
['link5.html']

6. 获取倒数第二个元素的内容
result = html.xpath('//li[last()-1]/a')
print result[0].text
运行结果
fourth item

7. 获取 class 为 bold 的标签名
result = html.xpath('//*[@class="bold"]')
print result[0].tag
运行结果
span

```
#### xpath进阶用法
``` python
# 对节点进行xpath查询 处理字段缺失
tables = html.xpath("//div[@class='indent']//table")
for table in tables:
            score = table.xpath("//span[@class='rating_nums']")[0].text
            description = "暂无简介"
            node = table.xpath("//span[@class='inq']")
            if node:
                description = node[0].text

# 获取某节点内所有文字内容
# <li class="item-1">sss <a href="link2.html">second item</a> ss</li>
res = html.xpath('//li')[0]

text = res.xpath('string(.)')
 html.xpath("string(//div/ul[@class='show'])")
print(l1)

# 结果： sss second item ss
```

### Beautiful Soup 的使用
```python
pip install beautifulsoup4
```
#### 常用API
**find_all( name , attrs , recursive , string , \*\*kwargs )** 返回所有匹配的节点

**find()** 只返回第一个匹配的节点
```python
from bs4 import BeautifulSoup
 
soup = BeautifulSoup(html_doc, 'lxml')

soup.find_all('div', class_='top')
# 这里注意下，class是Python的内部关键词，我们需要在css属性class后面加一个下划线'_'，不然会报错。

soup.find_all('a', id='link2')
# [<a id="link2" href="http://example.com/lacie">Lacie</a>]
```

#### css 选择器
```python
soup.select(".sister")
# 默认返回的都是列表形式
```

#### 提取标签内容
```python
for i in list:
    print(i.get_text()) # 我们使用get_text()方法获得标签内容
    print(i.get['href'] # get['attrs']方法获得标签属性
    print(i['href']) # 简写结果一样
```

# 相关教程
[爬虫教程](http://www.jqhtml.com/13259.html)

